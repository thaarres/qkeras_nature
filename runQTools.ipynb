{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "from qkeras import print_qstats\n",
    "\n",
    "# for automatic quantization\n",
    "import pprint\n",
    "from qkeras.autoqkeras import *\n",
    "from qkeras import *\n",
    "from qkeras.utils import model_quantize\n",
    "from qkeras.qtools import run_qtools, interface\n",
    "from qkeras.qtools import settings as qtools_settings\n",
    "from qkeras import quantized_bits\n",
    "from qkeras import QDense, QActivation\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "from qkeras import quantized_bits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('run_config.json', 'r') as f:\n",
    "    run_config = json.load(f)\n",
    "aq = autoqkeras.forgiving_metrics.ForgivingFactorBits(8, 8, 2, config=run_config['quantization_config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qe(image_size=16):\n",
    "#     inputs = tf.keras.Input((16),name=\"Input\")\n",
    "#     x = QDense(32, kernel_quantizer = quantized_bits(4,0,1), use_bias=False, name=\"qdense_1\")(inputs)\n",
    "#     x = QBatchNormalization()(x)\n",
    "#     x = QActivation('quantized_relu(4,2)',name=\"qact_1\")(x)\n",
    "    \n",
    "#     x = QDense(16, kernel_quantizer = 'ternary', use_bias=False, name=\"qdense_2\")(x)\n",
    "#     x = QBatchNormalization()(x)\n",
    "#     x = QActivation('quantized_relu(3,1)',name=\"qact_2\")(x)\n",
    "    \n",
    "#     x = QDense(16, kernel_quantizer = quantized_bits(2,1,1), use_bias=False, name=\"qdense_3\")(x)\n",
    "#     x = QBatchNormalization()(x)\n",
    "#     x = QActivation('quantized_relu(4,2)',name=\"qact_3\")(x)\n",
    "    \n",
    "#     x = QDense(5, kernel_quantizer = 'stochastic_binary', use_bias=False, name=\"qdense_nclasses\")(x)\n",
    "#     predictions = tf.keras.layers.Activation('softmax',name=\"softmax\")(x)\n",
    "#     model = tf.keras.Model(inputs, predictions,name='baseline')\n",
    "    model = tf.keras.models.load_model('QE.h5',custom_objects={'PruneLowMagnitude': pruning_wrapper.PruneLowMagnitude,'QDense': QDense, 'QConv2D': QConv2D, 'Clip': Clip, 'QActivation': QActivation})\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_qb(image_size=16):\n",
    "#     inputs = tf.keras.Input((16),name=\"Input\")\n",
    "#     x = QDense(32, kernel_quantizer = quantized_bits(4,0,1), use_bias=False, name=\"qdense_1\")(inputs)\n",
    "#     x = QBatchNormalization()(x)\n",
    "#     x = QActivation('quantized_relu(4,2)',name=\"qact_1\")(x)\n",
    "    \n",
    "#     x = QDense(16, kernel_quantizer = 'stochastic_binary', use_bias=False, name=\"qdense_2\")(x)\n",
    "#     x = QBatchNormalization()(x)\n",
    "#     x = QActivation('quantized_relu(3,1)',name=\"qact_2\")(x)\n",
    "    \n",
    "#     x = QDense(16, kernel_quantizer = 'stochastic_binary', use_bias=False, name=\"qdense_3\")(x)\n",
    "#     x = QBatchNormalization()(x)\n",
    "#     x = QActivation('quantized_relu(4,2)',name=\"qact_3\")(x)\n",
    "    \n",
    "#     x = QDense(5, kernel_quantizer = 'stochastic_binary', use_bias=False, name=\"qdense_nclasses\")(x)\n",
    "#     predictions = tf.keras.layers.Activation('softmax',name=\"softmax\")(x)\n",
    "#    model = tf.keras.Model(inputs, predictions,name='baseline')\n",
    "    model = tf.keras.models.load_model('QB.h5',custom_objects={'PruneLowMagnitude': pruning_wrapper.PruneLowMagnitude,'QDense': QDense, 'QConv2D': QConv2D, 'Clip': Clip, 'QActivation': QActivation})\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_q6(image_size=16):\n",
    "#     inputs = tf.keras.Input((16),name=\"Input\")\n",
    "#     x = QDense(64, kernel_quantizer = quantized_bits(6,0,1), bias_quantizer = quantized_bits(6,0,1))(inputs)\n",
    "#     x = QActivation('quantized_relu(6,0)',name=\"qact_1\")(x)\n",
    "#     x = QDense(32, kernel_quantizer = quantized_bits(6,0,1), bias_quantizer = quantized_bits(6,0,1))(x)\n",
    "#     x = QActivation('quantized_relu(6,0)',name=\"qact_2\")(x)\n",
    "#     x = QDense(32, kernel_quantizer = quantized_bits(6,0,1), bias_quantizer = quantized_bits(6,0,1))(x)\n",
    "#     x = QActivation('quantized_relu(6,0)',name=\"qact_3\")(x)\n",
    "#     x = QDense(5, kernel_quantizer = quantized_bits(6,0,1), bias_quantizer = quantized_bits(6,0,1))(x)\n",
    "#     predictions = tf.keras.layers.Activation('softmax',name=\"softmax\")(x)\n",
    "#     model = tf.keras.Model(inputs, predictions,name='baseline')\n",
    "    model = tf.keras.models.load_model('Q6.h5',custom_objects={'PruneLowMagnitude': pruning_wrapper.PruneLowMagnitude,'QDense': QDense, 'QConv2D': QConv2D, 'Clip': Clip, 'QActivation': QActivation})\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_bf(image_size=16):\n",
    "    inputs = tf.keras.Input((16),name=\"Input\")\n",
    "    x = QDense(64, kernel_quantizer = quantized_bits(15,5,1), bias_quantizer = quantized_bits(15,5,1), name=\"fc1\")(inputs)\n",
    "    x = QActivation('quantized_relu(15,5)',name=\"relu1\")(x)\n",
    "    x = QDense(32, kernel_quantizer = quantized_bits(15,5,1), bias_quantizer = quantized_bits(15,5,1), name=\"fc2\")(x)\n",
    "    x = QActivation('quantized_relu(15,5)',name=\"relu2\")(x)\n",
    "    x = QDense(32, kernel_quantizer = quantized_bits(15,5,1), bias_quantizer = quantized_bits(15,5,1), name=\"fc3\")(x)\n",
    "    x = QActivation('quantized_relu(15,5)',name=\"relu3\")(x)\n",
    "    x = QDense(5, kernel_quantizer = quantized_bits(15,5,1), bias_quantizer = quantized_bits(15,5,1), name=\"output\")(x)\n",
    "    predictions = tf.keras.layers.Activation('softmax',name=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs, predictions,name='baseline')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEstimates(model):\n",
    "    print('For model {}'.format(model.name))\n",
    "    q = run_qtools.QTools(model, \n",
    "                          process=\"horowitz\", \n",
    "                          source_quantizers=[quantized_bits(15, 5, 1)], \n",
    "                          is_inference=True, \n",
    "                          weights_path=None,\n",
    "                          keras_quantizer=\"fp16\",\n",
    "                          keras_accumulator=\"fp16\", \n",
    "                          for_reference=False)\n",
    "    total_bitsize = 0\n",
    "    total_parbitsize = 0\n",
    "    total_actbitsize = 0\n",
    "    perLayerBitSizes_param = {}\n",
    "    perLayerBitSizes_act = {}\n",
    "    for layer in model.layers:\n",
    "        if layer.name.find('nput')!=-1:\n",
    "            continue\n",
    "        layer_name = layer.__class__.__name__\n",
    "        parameters = aq._param_size(layer)\n",
    "        activations = aq._act_size(layer)\n",
    "        pprint.pprint('Layer = {} Parameter bitsize = {}'.format(layer.name,parameters))\n",
    "        pprint.pprint('Layer = {} Activation bitsize = {}'.format(layer.name,activations))\n",
    "        total_parbitsize  += parameters\n",
    "        total_actbitsize  += activations\n",
    "        perLayerBitSizes_param[layer.name] = parameters\n",
    "        perLayerBitSizes_act[layer.name]   = activations\n",
    "        total_bitsize += parameters+activations\n",
    "    print('\\n Total bit size = {} \\n'.format(total_bitsize))\n",
    "    print('Used for weights = {} \\n'.format(total_parbitsize))\n",
    "    print('Used for activations = {} \\n'.format(total_actbitsize))\n",
    "    # caculate energy of the derived data type map.\n",
    "    energy_dict = q.pe(\n",
    "        # whether to store parameters in dram, sram, or fixed\n",
    "        weights_on_memory=\"fixed\",\n",
    "        # store activations in dram or sram\n",
    "        activations_on_memory=\"fixed\",\n",
    "        # minimum sram size in number of bits. Let's assume a 16MB SRAM.\n",
    "        min_sram_size=8*16*1024*1024,\n",
    "        # whether load data from dram to sram (consider sram as a cache\n",
    "        # for dram. If false, we will assume data will be already in SRAM\n",
    "        rd_wr_on_io=False)\n",
    "    # get stats of energy distribution in each layer\n",
    "    energy_profile = q.extract_energy_profile(\n",
    "        qtools_settings.cfg.include_energy, energy_dict)\n",
    "    # extract sum of energy of each layer according to the rule specified in\n",
    "    # qtools_settings.cfg.include_energy\n",
    "    total_energy = q.extract_energy_sum(\n",
    "        qtools_settings.cfg.include_energy, energy_dict)\n",
    "    perLayerEnergy = {}\n",
    "    for key in energy_profile.keys():\n",
    "        pprint.pprint('Layer = {} Energy = {} pJ'.format(key, energy_profile[key]['energy']['op_cost'] ))\n",
    "        perLayerEnergy[key] = energy_profile[key]['energy']['op_cost']\n",
    "    print()\n",
    "    print(\"\\n Total energy: {:.1f} pJ \\n\".format(total_energy))\n",
    "    return total_bitsize, perLayerBitSizes_param, perLayerBitSizes_act, total_energy, perLayerEnergy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bitsize       = {}\n",
    "perLayerBitSize_par = {} \n",
    "perLayerBitSize_act = {} \n",
    "total_energy        = {} \n",
    "perLayerEnergy      = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model baseline\n",
      "'Layer = fc1 Parameter bitsize = 16320'\n",
      "'Layer = fc1 Activation bitsize = 0'\n",
      "'Layer = relu1 Parameter bitsize = 0'\n",
      "'Layer = relu1 Activation bitsize = 960'\n",
      "'Layer = fc2 Parameter bitsize = 31200'\n",
      "'Layer = fc2 Activation bitsize = 0'\n",
      "'Layer = relu2 Parameter bitsize = 0'\n",
      "'Layer = relu2 Activation bitsize = 480'\n",
      "'Layer = fc3 Parameter bitsize = 15840'\n",
      "'Layer = fc3 Activation bitsize = 0'\n",
      "'Layer = relu3 Parameter bitsize = 0'\n",
      "'Layer = relu3 Activation bitsize = 480'\n",
      "'Layer = output Parameter bitsize = 2475'\n",
      "'Layer = output Activation bitsize = 0'\n",
      "'Layer = softmax Parameter bitsize = 0'\n",
      "'Layer = softmax Activation bitsize = 40'\n",
      "\n",
      " Total bit size = 67795 \n",
      "\n",
      "Used for weights = 65835 \n",
      "\n",
      "Used for activations = 1960 \n",
      "\n",
      "'Layer = fc1 Energy = 818.0 pJ'\n",
      "'Layer = relu1 Energy = 0.0 pJ'\n",
      "'Layer = fc2 Energy = 1648.8 pJ'\n",
      "'Layer = relu2 Energy = 0.0 pJ'\n",
      "'Layer = fc3 Energy = 821.2 pJ'\n",
      "'Layer = relu3 Energy = 0.0 pJ'\n",
      "'Layer = output Energy = 128.31 pJ'\n",
      "'Layer = softmax Energy = 0.0 pJ'\n",
      "\n",
      "\n",
      " Total energy: 3439.0 pJ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_bf()\n",
    "total_bitsize['BF'], perLayerBitSize_par['BF'], perLayerBitSize_act['BF'], total_energy['BF'], perLayerEnergy['BF'] = getEstimates(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model model\n",
      "'Layer = fc1 Parameter bitsize = 6528'\n",
      "'Layer = fc1 Activation bitsize = 0'\n",
      "'Layer = relu1 Parameter bitsize = 0'\n",
      "'Layer = relu1 Activation bitsize = 384'\n",
      "'Layer = fc2 Parameter bitsize = 12480'\n",
      "'Layer = fc2 Activation bitsize = 0'\n",
      "'Layer = relu2 Parameter bitsize = 0'\n",
      "'Layer = relu2 Activation bitsize = 192'\n",
      "'Layer = fc3 Parameter bitsize = 6336'\n",
      "'Layer = fc3 Activation bitsize = 0'\n",
      "'Layer = relu3 Parameter bitsize = 0'\n",
      "'Layer = relu3 Activation bitsize = 192'\n",
      "'Layer = output Parameter bitsize = 990'\n",
      "'Layer = output Activation bitsize = 0'\n",
      "'Layer = softmax Parameter bitsize = 0'\n",
      "'Layer = softmax Activation bitsize = 40'\n",
      "\n",
      " Total bit size = 27142 \n",
      "\n",
      "Used for weights = 26334 \n",
      "\n",
      "Used for activations = 808 \n",
      "\n",
      "'Layer = fc1 Energy = 369.32 pJ'\n",
      "'Layer = relu1 Energy = 0.0 pJ'\n",
      "'Layer = fc2 Energy = 355.2 pJ'\n",
      "'Layer = relu2 Energy = 0.0 pJ'\n",
      "'Layer = fc3 Energy = 174.4 pJ'\n",
      "'Layer = relu3 Energy = 0.0 pJ'\n",
      "'Layer = output Energy = 27.25 pJ'\n",
      "'Layer = softmax Energy = 0.0 pJ'\n",
      "\n",
      "\n",
      " Total energy: 949.0 pJ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_q6()\n",
    "total_bitsize['Q6'], perLayerBitSize_par['Q6'], perLayerBitSize_act['Q6'], total_energy['Q6'], perLayerEnergy['Q6'] = getEstimates(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model model\n",
      "'Layer = fc1 Parameter bitsize = 2048'\n",
      "'Layer = fc1 Activation bitsize = 0'\n",
      "'Layer = bn1 Parameter bitsize = 512'\n",
      "'Layer = bn1 Activation bitsize = 0'\n",
      "'Layer = relu1 Parameter bitsize = 0'\n",
      "'Layer = relu1 Activation bitsize = 128'\n",
      "'Layer = fc2 Parameter bitsize = 512'\n",
      "'Layer = fc2 Activation bitsize = 0'\n",
      "'Layer = bn2 Parameter bitsize = 256'\n",
      "'Layer = bn2 Activation bitsize = 0'\n",
      "'Layer = relu2 Parameter bitsize = 0'\n",
      "'Layer = relu2 Activation bitsize = 64'\n",
      "'Layer = fc3 Parameter bitsize = 512'\n",
      "'Layer = fc3 Activation bitsize = 0'\n",
      "'Layer = bn3 Parameter bitsize = 256'\n",
      "'Layer = bn3 Activation bitsize = 0'\n",
      "'Layer = relu3 Parameter bitsize = 0'\n",
      "'Layer = relu3 Activation bitsize = 48'\n",
      "'Layer = output_softmax Parameter bitsize = 120'\n",
      "'Layer = output_softmax Activation bitsize = 0'\n",
      "'Layer = softmax Parameter bitsize = 0'\n",
      "'Layer = softmax Activation bitsize = 40'\n",
      "\n",
      " Total bit size = 4496 \n",
      "\n",
      "Used for weights = 4216 \n",
      "\n",
      "Used for activations = 280 \n",
      "\n",
      "'Layer = fc1 Energy = 134.53 pJ'\n",
      "'Layer = bn1 Energy = 70.4 pJ'\n",
      "'Layer = relu1 Energy = 0.0 pJ'\n",
      "'Layer = fc2 Energy = 12.16 pJ'\n",
      "'Layer = bn2 Energy = 35.2 pJ'\n",
      "'Layer = relu2 Energy = 0.0 pJ'\n",
      "'Layer = fc3 Energy = 9.92 pJ'\n",
      "'Layer = bn3 Energy = 35.2 pJ'\n",
      "'Layer = relu3 Energy = 0.0 pJ'\n",
      "'Layer = output_softmax Energy = 1.65 pJ'\n",
      "'Layer = softmax Energy = 0.0 pJ'\n",
      "\n",
      "\n",
      " Total energy: 181.0 pJ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_qb()\n",
    "for layer in model.layers:\n",
    "    layer._name = layer.name.replace('_relu','')\n",
    "total_bitsize['QB'], perLayerBitSize_par['QB'], perLayerBitSize_act['QB'], total_energy['QB'], perLayerEnergy['QB'] = getEstimates(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model model\n",
      "'Layer = fc1 Parameter bitsize = 2048'\n",
      "'Layer = fc1 Activation bitsize = 0'\n",
      "'Layer = bn1 Parameter bitsize = 512'\n",
      "'Layer = bn1 Activation bitsize = 0'\n",
      "'Layer = relu1 Parameter bitsize = 0'\n",
      "'Layer = relu1 Activation bitsize = 128'\n",
      "'Layer = fc2 Parameter bitsize = 1024'\n",
      "'Layer = fc2 Activation bitsize = 0'\n",
      "'Layer = bn2 Parameter bitsize = 256'\n",
      "'Layer = bn2 Activation bitsize = 0'\n",
      "'Layer = relu2 Parameter bitsize = 0'\n",
      "'Layer = relu2 Activation bitsize = 48'\n",
      "'Layer = fc3 Parameter bitsize = 512'\n",
      "'Layer = fc3 Activation bitsize = 0'\n",
      "'Layer = bn3 Parameter bitsize = 256'\n",
      "'Layer = bn3 Activation bitsize = 0'\n",
      "'Layer = relu3 Parameter bitsize = 0'\n",
      "'Layer = relu3 Activation bitsize = 64'\n",
      "'Layer = output_softmax Parameter bitsize = 120'\n",
      "'Layer = output_softmax Activation bitsize = 0'\n",
      "'Layer = softmax Parameter bitsize = 0'\n",
      "'Layer = softmax Activation bitsize = 40'\n",
      "\n",
      " Total bit size = 5008 \n",
      "\n",
      "Used for weights = 4728 \n",
      "\n",
      "Used for activations = 280 \n",
      "\n",
      "'Layer = fc1 Energy = 134.53 pJ'\n",
      "'Layer = bn1 Energy = 70.4 pJ'\n",
      "'Layer = relu1 Energy = 0.0 pJ'\n",
      "'Layer = fc2 Energy = 21.44 pJ'\n",
      "'Layer = bn2 Energy = 35.2 pJ'\n",
      "'Layer = relu2 Energy = 0.0 pJ'\n",
      "'Layer = fc3 Energy = 13.25 pJ'\n",
      "'Layer = bn3 Energy = 35.2 pJ'\n",
      "'Layer = relu3 Energy = 0.0 pJ'\n",
      "'Layer = output_softmax Energy = 1.65 pJ'\n",
      "'Layer = softmax Energy = 0.0 pJ'\n",
      "\n",
      "\n",
      " Total energy: 193.0 pJ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_qe()\n",
    "for layer in model.layers:\n",
    "    layer._name = layer.name.replace('_relu','')\n",
    "total_bitsize['QE'], perLayerBitSize_par['QE'], perLayerBitSize_act['QE'], total_energy['QE'], perLayerEnergy['QE'] = getEstimates(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Model      Bits   Energy [pJ] Bits/BF Bits/Q6 Energy/BF Energy/Q6'\n",
      "'BF QTools 67795     3439       1.00     2.50     1.00     3.62'\n",
      "'BF Vivado -         -          -          -      1.00   5.81'\n",
      "''\n",
      "'Q6 QTools 27142     949       0.40     1.00     0.28     1.00'\n",
      "'Q6 Vivado -         -          -          -      0.17   1.00'\n",
      "''\n",
      "'QE QTools 5008     193       0.07     0.18     0.06     0.20'\n",
      "'QE Vivado -         -          -          -      0.05   0.30'\n",
      "''\n",
      "'QB QTools 4496     181       0.07     0.17     0.05     0.19'\n",
      "'QB Vivado -         -          -          -      0.04   0.25'\n",
      "''\n"
     ]
    }
   ],
   "source": [
    "#Let's also look at the relative energies and bit sizes per layer\n",
    "models = ['BF','Q6','QE','QB']\n",
    "pprint.pprint('{}      {}   {} {} {} {} {}'.format('Model', 'Bits', 'Energy [pJ]', 'Bits/BF', 'Bits/Q6', 'Energy/BF', 'Energy/Q6'))\n",
    "for i,model in enumerate(models):\n",
    "    pprint.pprint('{} {:.0f}     {:.0f}       {:.2f}     {:.2f}     {:.2f}     {:.2f}'.format('{} QTools'.format(model), total_bitsize[model], total_energy[model], total_bitsize[model]/total_bitsize['BF'], total_bitsize[model]/total_bitsize['Q6'], total_energy[model]/total_energy['BF'], total_energy[model]/total_energy['Q6']))\n",
    "    pprint.pprint('{} {}         {}          {}          {}      {:.2f}   {:.2f}'.format('{} Vivado'.format(model), '-', '-', '-', '-', vivadoEBF[i], vivadoEQ6[i])) \n",
    "    pprint.pprint('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " For model BF\n",
      "bits/Q6 = {'fc1': 2.5, 'fc2': 2.5, 'fc3': 2.5, 'output': 2.5}\n",
      "bits/BF = {'fc1': 1.0, 'fc2': 1.0, 'fc3': 1.0, 'output': 1.0}\n",
      "ener/Q6 = {'fc1': 2.2148814036607822, 'fc2': 4.641891891891892, 'fc3': 4.708715596330276, 'output': 4.70862385321101}\n",
      "ener/BF = {'fc1': 1.0, 'fc2': 1.0, 'fc3': 1.0, 'output': 1.0}\n",
      "\n",
      " For model Q6\n",
      "bits/Q6 = {'fc1': 1.0, 'fc2': 1.0, 'fc3': 1.0, 'output': 1.0}\n",
      "bits/BF = {'fc1': 0.4, 'fc2': 0.4, 'fc3': 0.4, 'output': 0.4}\n",
      "ener/Q6 = {'fc1': 1.0, 'fc2': 1.0, 'fc3': 1.0, 'output': 1.0}\n",
      "ener/BF = {'fc1': 0.4514914425427873, 'fc2': 0.21542940320232898, 'fc3': 0.21237213833414514, 'output': 0.21237627620606345}\n",
      "\n",
      " For model QE\n",
      "bits/Q6 = {'fc1': 0.3137254901960784, 'fc2': 0.08205128205128205, 'fc3': 0.08080808080808081}\n",
      "bits/BF = {'fc1': 0.12549019607843137, 'fc2': 0.03282051282051282, 'fc3': 0.03232323232323232}\n",
      "ener/Q6 = {'fc1': 0.3642640528538936, 'fc2': 0.06036036036036037, 'fc3': 0.07597477064220183}\n",
      "ener/BF = {'fc1': 0.16446210268948655, 'fc2': 0.013003396409509947, 'fc3': 0.016134924500730636}\n",
      "\n",
      " For model QB\n",
      "bits/Q6 = {'fc1': 0.3137254901960784, 'fc2': 0.041025641025641026, 'fc3': 0.08080808080808081}\n",
      "bits/BF = {'fc1': 0.12549019607843137, 'fc2': 0.01641025641025641, 'fc3': 0.03232323232323232}\n",
      "ener/Q6 = {'fc1': 0.3642640528538936, 'fc2': 0.03423423423423424, 'fc3': 0.056880733944954125}\n",
      "ener/BF = {'fc1': 0.16446210268948655, 'fc2': 0.007375060650169821, 'fc3': 0.012079883097905503}\n"
     ]
    }
   ],
   "source": [
    "#Let's look at the relative energies and bit sizes per layer\n",
    "models = ['BF','Q6','QE','QB']\n",
    "\n",
    "for i,model in enumerate(models):\n",
    "    print('\\n For model {}'.format(model))\n",
    "    perLayerBrelQ6 = {}\n",
    "    perLayerBrelBF = {}\n",
    "    perLaterErelQ6 = {}\n",
    "    perLaterErelBF = {}\n",
    "    for key in perLayerBitSize[model]:\n",
    "        if key.find('relu')!=-1 or key.find('softmax')!=-1 or key.find('bn')!=-1:\n",
    "            continue\n",
    "        perLayerBrelQ6[key] = perLayerBitSize[model][key]/perLayerBitSize['Q6'][key]\n",
    "        perLayerBrelBF[key] = perLayerBitSize[model][key]/perLayerBitSize['BF'][key]\n",
    "    for key in perLayerEnergy[model]:\n",
    "        if key.find('relu')!=-1 or key.find('softmax')!=-1 or key.find('bn')!=-1:\n",
    "            continue\n",
    "        perLaterErelQ6[key] = perLayerEnergy[model][key]/perLayerEnergy['Q6'][key]\n",
    "        perLaterErelBF[key] = perLayerEnergy[model][key]/perLayerEnergy['BF'][key]   \n",
    "    print('bits/Q6 = {}'.format(perLayerBrelQ6))\n",
    "    print('bits/BF = {}'.format(perLayerBrelBF))\n",
    "    print('ener/Q6 = {}'.format(perLaterErelQ6))\n",
    "    print('ener/BF = {}'.format(perLaterErelBF))\n",
    "    #pprint.pprint('{} {:.0f}     {:.0f}       {:.2f}     {:.2f}     {:.2f}     {:.2f}'.format('{} QTools'.format(model), total_bitsize[model], total_energy[model], total_bitsize[model]/total_bitsize['BF'], total_bitsize[model]/total_bitsize['Q6'], total_energy[model]/total_energy['BF'], total_energy[model]/total_energy['Q6']))\n",
    "    #pprint.pprint('{} {}         {}          {}          {}      {:.2f}   {:.2f}'.format('{} Vivado'.format(model), '-', '-', '-', '-', vivadoEBF[i], vivadoEQ6[i])) \n",
    "    #pprint.pprint('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
